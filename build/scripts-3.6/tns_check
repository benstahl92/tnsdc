#!/anaconda3/bin/python

# standard imports
import pandas as pd
from astropy.coordinates import Distance
from tabulate import tabulate
import os

# custom imports
from tnsdc import *

# -----------------------------------------------------------------------------------
# TNS Discovery Checker

# intended to be run daily as a script by a cron job
# candidate discoveries are pulled from TNS, processed, and posted to Slack
# expects a file called slack.webhook to exist in working directory that contains url only

# -----------------------------------------------------------------------------------
# default values (global)
search_buffer = 2 # days to go back from today in search
db_buffer = 15 # days to go back from today if making database for the first time

# -----------------------------------------------------------------------------------
# main function

def main(day_buffer, db):
	'''
	check TNS for new discoveries (compared against a local reference database)
	for new discoveries:
		get redshift from NED
		calculate absolute magnitude
	post summary to Slack channel
	'''

	# download discoveries from past <day_buffer> days
	end = datetime.now()
	start = end - timedelta(days = day_buffer)
	new = scrape_TNS(start, end)

	# calculate redshift and absolute magnitude of discoveries
	new['z_NED'] = new.apply(lambda row: get_z_ned(RA = row['RA'], DEC = row['DEC']), axis = 1)
	selector = new['z_NED'].notnull() & (new['z_NED'] > 0) & new['Discovery Mag'].notnull()
	new.loc[selector, 'Abs Mag'] = new.loc[selector, 'Discovery Mag'] * u.mag - Distance(z = new.loc[selector, 'z_NED']).distmod

	if not os.path.exists(db):
		# write database
		new.to_csv(db)
	else:
		# compare to database to determine truly new discoveries and then update database
		ref = pd.read_csv(db, index_col = 0)
		new = new.loc[~new['ID'].isin(ref['ID']), :]
		ref = pd.concat([ref, new], axis = 0)
		ref.to_csv(db)

		# post Slack announcement
		alert_msg = 'Daily TNS Report {}\n'.format('{}-{:02}-{:02}'.format(end.year, end.month, end.day))
		alert_msg += tabulate(new.loc[:,['ID', 'Name', 'RA', 'DEC', 'Discovering Group/s', 'Discovery Date (UT)', 'z_NED', 'Discovery Mag', 'Abs Mag']], 
					 showindex = False, headers = 'keys', tablefmt = 'psql')
		if not os.path.exists('slack.webhook'):
			print('file slack.webhook does not exist --- printing message instead')
			print(alert_msg)
			return
		with open('slack.webhook', 'r') as f:
			url = f.read().rstrip()
		slack_alert(alert_msg, url)

# -----------------------------------------------------------------------------------
# script

if __name__ == '__main__':

	# collect arguments
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('db', type = str, help = 'name of database (a simple csv file, which will be created if it does not exist)')
    parser.add_argument('-b', '--buffer in days', dest = 'buffer', type = float, default = search_buffer, help = 'number of days in past to retrieve')
    args = parser.parse_args()

    # check if database needs to be made and set buffer if necessary
    if not os.path.exists(args.db):
    	print('making database, this may take some time')
    	if args.buffer == search_buffer:
    		args.buffer = db_buffer

    main(args.buffer, args.db)

